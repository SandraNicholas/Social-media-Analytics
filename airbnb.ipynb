{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Using cached symspellpy-6.7.6-py3-none-any.whl (2.6 MB)\n",
      "Collecting editdistpy>=0.1.3\n",
      "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: editdistpy\n",
      "  Building wheel for editdistpy (PEP 517): started\n",
      "  Building wheel for editdistpy (PEP 517): finished with status 'done'\n",
      "  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp39-cp39-win_amd64.whl size=30725 sha256=50ea46867c4cc55ddf5a702d9807e0532abfd9c1fab28b8a0f1a7d9c8b3e7a17\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\72\\16\\b1\\ea6472dedd6ac13ad789dd294871a76fd6906fca3c010347d1\n",
      "Successfully built editdistpy\n",
      "Installing collected packages: editdistpy, symspellpy\n",
      "Successfully installed editdistpy-0.1.3 symspellpy-6.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.6.1-py3-none-win_amd64.whl (125.4 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\new folder\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\new folder\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "!pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\user\\new folder\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Using cached wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\new folder\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\new folder\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Using cached spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Using cached typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\new folder\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\new folder\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\new folder\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\new folder\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\new folder\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\user\\new folder\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\new folder\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\new folder\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\new folder\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\new folder\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\new folder\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 typer-0.4.1 wasabi-0.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "!pip install spacy\n",
    "import spacy\n",
    "#from TokenizationTest import TokenizationTest\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting happiestfuntokenizing\n",
      "  Using cached happiestfuntokenizing-0.0.7.tar.gz (6.3 kB)\n",
      "Building wheels for collected packages: happiestfuntokenizing\n",
      "  Building wheel for happiestfuntokenizing (setup.py): started\n",
      "  Building wheel for happiestfuntokenizing (setup.py): finished with status 'done'\n",
      "  Created wheel for happiestfuntokenizing: filename=happiestfuntokenizing-0.0.7-py3-none-any.whl size=6727 sha256=ac8ce7773d5d8ed35998451c7ea3e71c429c46c630261ae56a70b586a9770aef\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\ad\\7d\\48\\629285105629369c5479a96a8b49171413effa807ca7a13a0b\n",
      "Successfully built happiestfuntokenizing\n",
      "Installing collected packages: happiestfuntokenizing\n",
      "Successfully installed happiestfuntokenizing-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install happiestfuntokenizing\n",
    "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import re, string, json\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import  cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data \n",
    "Tokenization, Lemmatization, Stemming \n",
    "\n",
    "1.   Function regextokenizer was used to remove punctuations,special characters, @ and white spaces\n",
    "2.   Function Spell_Checker was used to correct the spellings of the words and normalize\n",
    "3.Function normalize_contractions was used to remove contractions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexptokenizer(data):\n",
    "  data['text'] =  data['text'].map(lambda x:BeautifulSoup(x, 'lxml').get_text())\n",
    "  #remove words which are starts with @ symbols\n",
    "  data['text'] = data['text'].map(lambda x:re.sub('@\\w*','',str(x)))\n",
    "  #remove link starts with https\n",
    "  #simplify_punctuation  \n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r'([!?,;])\\1+', r'\\1', str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r'\\.{2,}', r'...', str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:re.sub('http.*','',str(x)))\n",
    "  #removing data and time (numeric values)\n",
    "  data['text'] = data['text'].map(lambda x:re.sub('[0-9]','',str(x)))\n",
    "  #removing special characters\n",
    "  data['text'] = data['text'].map(lambda x:re.sub('[#|*|$|:|\\\\|&|]','',str(x)))\n",
    "  #normalizing whitespace\n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r\"//t\",r\"\\t\", str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r\"( )\\1+\",r\"\\1\", str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r\"(\\n)\\1+\",r\"\\1\", str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r\"(\\r)\\1+\",r\"\\1\", str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:re.sub(r\"(\\t)\\1+\",r\"\\1\", str(x)))\n",
    "  data['text'] = data['text'].map(lambda x:unidecode.unidecode(str(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_contractions(sentence_list):\n",
    "    contraction_list = json.loads(open('english_contractions.json', 'r').read())\n",
    "    norm_sents = []\n",
    "    print(\"Normalizing contractions\")\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        norm_sents.append(_normalize_contractions_text(sentence, contraction_list))\n",
    "    return norm_sents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_contractions_text(text, contractions):\n",
    "    \"\"\"\n",
    "    This function normalizes english contractions.\n",
    "    \"\"\"\n",
    "    new_token_list = []\n",
    "    token_list = text.split()\n",
    "    for word_pos in range(len(token_list)):\n",
    "        word = token_list[word_pos]\n",
    "        first_upper = False\n",
    "        if word[0].isupper():\n",
    "            first_upper = True\n",
    "        if word.lower() in contractions:\n",
    "            replacement = contractions[word.lower()]\n",
    "            if first_upper:\n",
    "                replacement = replacement[0].upper()+replacement[1:]\n",
    "            replacement_tokens = replacement.split()\n",
    "            if len(replacement_tokens)>1:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "                new_token_list.append(replacement_tokens[1])\n",
    "            else:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "        else:\n",
    "            new_token_list.append(word)\n",
    "    sentence = \" \".join(new_token_list).strip(\" \")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(sentence_list):\n",
    "    max_edit_distance_dictionary= 3\n",
    "    prefix_length = 4\n",
    "    spellchecker = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    spellchecker.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    spellchecker.load_bigram_dictionary(dictionary_path, term_index=0, count_index=2)\n",
    "    norm_sents = []\n",
    "    print(\"Spell correcting\")\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        norm_sents.append(_spell_correction_text(sentence, spellchecker))\n",
    "    return norm_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(text):\n",
    "    for char in text:\n",
    "        if not (char in \"0123456789\" or char in \",%.$\"):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce_exaggerations(text):\n",
    "    \"\"\"\n",
    "    Auxiliary function to help with exxagerated words.\n",
    "    Examples:\n",
    "        woooooords -> words\n",
    "        yaaaaaaaaaaaaaaay -> yay\n",
    "    \"\"\"\n",
    "    correction = str(text)\n",
    "    #TODO work on complexity reduction.\n",
    "    return re.sub(r'([\\w])\\1+', r'\\1', correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech_tags(token):\n",
    "\n",
    "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
    "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenizer(data):\n",
    "  return str.split(data)\n",
    "\n",
    "def potter_tokenizer(data):\n",
    "  tokenizer= potts()\n",
    "  return tokenizer.tokenize(data)\n",
    "\n",
    "def spacy_tokenizer(data):\n",
    "  spacy_tokens=nlp(data)\n",
    "  return [token.text for token in spacy_tokens]\n",
    "\n",
    "def spacy_lemmatizer(data):\n",
    "  spacy_tokens=nlp(data)\n",
    "  tokens = [token.lemma_ for token in spacy_tokens]\n",
    "  #tokens = [unidecode.unidecode(accented_string) for token in tokens]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_treebank_tokenizer(data):\n",
    "  tokenizer= TreebankWordTokenizer()\n",
    "  return tokenizer.tokenize(data)\n",
    "\n",
    "\n",
    "def nltk_tweet_tokenizer(data):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(data)\n",
    "\n",
    "def nltk_twitter_stemmer_tokenizer(data):\n",
    "  tokenizer= TwitterTokenizer()\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens= tokenizer.tokenize(data)\n",
    "  return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def nltk_twitter_lemmatizer_tokenizer(data):\n",
    "  tokenizer= TweetTokenizer()\n",
    "  lemmatizer = WordNetLemmatizer()  \n",
    "  tokens= tokenizer.tokenize(data)\n",
    "  tokens = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens]   \n",
    "  return tokens\n",
    "\n",
    "def nltk_treebank_stemmer_tokenizer(data):\n",
    "  tokenizer= TreebankWordTokenizer()\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens= tokenizer.tokenize(data)\n",
    "  return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def nltk_treebank_lemmatizer_tokenizer(data):\n",
    "  tokenizer= TreebankWordTokenizer()\n",
    "  lemmatizer = WordNetLemmatizer()  \n",
    "  tokens= tokenizer.tokenize(data)\n",
    "  return [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(df, tokenizer):\n",
    "    corpus=[]\n",
    "    Y=[]\n",
    "    for idx in range(len(train_df)):\n",
    "        label=df['Target'][idx]\n",
    "        text=df['text'][idx]\n",
    "        tokens=' '.join(tokenizer(text))\n",
    "        corpus.append(tokens)\n",
    "        Y.append(label)\n",
    "    return corpus, Y\n",
    "\n",
    "\n",
    "def read_test_data(df, tokenizer):\n",
    "    corpus=[]\n",
    "    for idx in range(len(train_df)):\n",
    "        text=df['text'][idx]\n",
    "        tokens=' '.join(tokenizer(text))\n",
    "        corpus.append(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _spell_correction_text(text, spellchecker):\n",
    "    \"\"\"\n",
    "    This function does very simple spell correction normalization using pyspellchecker module. \n",
    "    It works over a tokenized sentence and only the token representations are changed.\n",
    "    \"\"\"\n",
    "    if len(text) < 1:\n",
    "        return \"\"\n",
    "    #Spell checker config\n",
    "    max_edit_distance_lookup = 2\n",
    "    suggestion_verbosity = Verbosity.TOP # TOP, CLOSEST, ALL\n",
    "    #End of Spell checker config\n",
    "    token_list = text.split()\n",
    "    for word_pos in range(len(token_list)):\n",
    "        word = token_list[word_pos]\n",
    "        if word is None:\n",
    "            token_list[word_pos] = \"\"\n",
    "            continue\n",
    "        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word.lower() in spellchecker.words.keys()):\n",
    "            suggestions = spellchecker.lookup(word.lower(), suggestion_verbosity, max_edit_distance_lookup)\n",
    "            #Checks first uppercase to conserve the case.\n",
    "            upperfirst = word[0].isupper()\n",
    "            #Checks for correction suggestions.\n",
    "            if len(suggestions) > 0:\n",
    "                correction = suggestions[0].term\n",
    "                replacement = correction\n",
    "            #We call our _reduce_exaggerations function if no suggestion is found. Maybe there are repeated chars.\n",
    "            else:\n",
    "                replacement = _reduce_exaggerations(word)\n",
    "            #Takes the case back to the word.\n",
    "            if upperfirst:\n",
    "                replacement = replacement[0].upper()+replacement[1:]\n",
    "            word = replacement\n",
    "            token_list[word_pos] = word\n",
    "    return \" \".join(token_list).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>amenities</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>thumbnail_url</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3895911</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>{TV,\"Cable TV\",Kitchen,\"Free parking on premis...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>flexible</td>\n",
       "      <td>True</td>\n",
       "      <td>LA</td>\n",
       "      <td>...</td>\n",
       "      <td>34.028372</td>\n",
       "      <td>-118.494449</td>\n",
       "      <td>Santa Monica Private Bedroom/Bathroom Suite</td>\n",
       "      <td>Santa Monica</td>\n",
       "      <td>6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/92355eae-b...</td>\n",
       "      <td>90403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9710289</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>{TV,\"Cable TV\",\"Wireless Internet\",\"Air condit...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>40.720380</td>\n",
       "      <td>-73.942329</td>\n",
       "      <td>Bright, charming luxury 1 BR with amazing rooftop</td>\n",
       "      <td>Williamsburg</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/da03e413-d...</td>\n",
       "      <td>11222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9051635</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>{\"Wireless Internet\",Kitchen,Heating,\"Family/k...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>SF</td>\n",
       "      <td>...</td>\n",
       "      <td>37.785434</td>\n",
       "      <td>-122.470284</td>\n",
       "      <td>Private room in charming apartment</td>\n",
       "      <td>Richmond District</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/0ba7d8aa-9...</td>\n",
       "      <td>94118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>708374</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>{TV,\"Cable TV\",Internet,\"Wireless Internet\",\"W...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>True</td>\n",
       "      <td>LA</td>\n",
       "      <td>...</td>\n",
       "      <td>33.976026</td>\n",
       "      <td>-118.463471</td>\n",
       "      <td>Marina del Rey Beach Jr 1 Bdrm 5</td>\n",
       "      <td>Marina Del Rey</td>\n",
       "      <td>7</td>\n",
       "      <td>94.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/30279741/4...</td>\n",
       "      <td>90292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>626296</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>{TV,Internet,\"Wireless Internet\",\"Air conditio...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>flexible</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>40.735573</td>\n",
       "      <td>-74.005996</td>\n",
       "      <td>Bright Studio Loft Prime Location</td>\n",
       "      <td>West Village</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/9384e262-8...</td>\n",
       "      <td>10014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25453</th>\n",
       "      <td>13098256</td>\n",
       "      <td>House</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>{TV,Internet,\"Wireless Internet\",Kitchen,\"Free...</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>True</td>\n",
       "      <td>SF</td>\n",
       "      <td>...</td>\n",
       "      <td>37.739624</td>\n",
       "      <td>-122.414839</td>\n",
       "      <td>Sunny Home in Prime Bernal Heights</td>\n",
       "      <td>Bernal Heights</td>\n",
       "      <td>8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/13111646/d...</td>\n",
       "      <td>94110</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25454</th>\n",
       "      <td>13550830</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>{Internet,\"Wireless Internet\",\"Air conditionin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>flexible</td>\n",
       "      <td>False</td>\n",
       "      <td>LA</td>\n",
       "      <td>...</td>\n",
       "      <td>34.148955</td>\n",
       "      <td>-118.368865</td>\n",
       "      <td>Luxury Apartment Toluca Lake</td>\n",
       "      <td>Toluca Lake</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25455</th>\n",
       "      <td>13899483</td>\n",
       "      <td>House</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>{TV,\"Wireless Internet\",\"Air conditioning\",Kit...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>LA</td>\n",
       "      <td>...</td>\n",
       "      <td>34.086756</td>\n",
       "      <td>-118.353835</td>\n",
       "      <td>WeHo House with Private Yard and Parking</td>\n",
       "      <td>West Hollywood</td>\n",
       "      <td>11</td>\n",
       "      <td>95.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/12249249-f...</td>\n",
       "      <td>90046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25456</th>\n",
       "      <td>9939029</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>{TV,Internet,\"Wireless Internet\",\"Air conditio...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>40.760537</td>\n",
       "      <td>-73.989708</td>\n",
       "      <td>Large 1BR in the Heart of Hell's Kitchen!!</td>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/f0ef6496-e...</td>\n",
       "      <td>10036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25457</th>\n",
       "      <td>12986510</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>{TV,Internet,\"Wireless Internet\",Kitchen,\"Buzz...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>False</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>40.688196</td>\n",
       "      <td>-73.945351</td>\n",
       "      <td>Room w/ Private Bath in Large Apt.</td>\n",
       "      <td>Bedford-Stuyvesant</td>\n",
       "      <td>4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>https://a0.muscache.com/im/pictures/96846ebf-b...</td>\n",
       "      <td>11221</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25458 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id property_type        room_type  \\\n",
       "0       3895911     Apartment     Private room   \n",
       "1       9710289     Apartment  Entire home/apt   \n",
       "2       9051635     Apartment     Private room   \n",
       "3        708374     Apartment  Entire home/apt   \n",
       "4        626296     Apartment  Entire home/apt   \n",
       "...         ...           ...              ...   \n",
       "25453  13098256         House  Entire home/apt   \n",
       "25454  13550830     Apartment     Private room   \n",
       "25455  13899483         House  Entire home/apt   \n",
       "25456   9939029     Apartment  Entire home/apt   \n",
       "25457  12986510     Apartment     Private room   \n",
       "\n",
       "                                               amenities  accommodates  \\\n",
       "0      {TV,\"Cable TV\",Kitchen,\"Free parking on premis...             2   \n",
       "1      {TV,\"Cable TV\",\"Wireless Internet\",\"Air condit...             3   \n",
       "2      {\"Wireless Internet\",Kitchen,Heating,\"Family/k...             1   \n",
       "3      {TV,\"Cable TV\",Internet,\"Wireless Internet\",\"W...             1   \n",
       "4      {TV,Internet,\"Wireless Internet\",\"Air conditio...             2   \n",
       "...                                                  ...           ...   \n",
       "25453  {TV,Internet,\"Wireless Internet\",Kitchen,\"Free...             6   \n",
       "25454  {Internet,\"Wireless Internet\",\"Air conditionin...             1   \n",
       "25455  {TV,\"Wireless Internet\",\"Air conditioning\",Kit...             2   \n",
       "25456  {TV,Internet,\"Wireless Internet\",\"Air conditio...             4   \n",
       "25457  {TV,Internet,\"Wireless Internet\",Kitchen,\"Buzz...             2   \n",
       "\n",
       "       bathrooms  bed_type cancellation_policy  cleaning_fee city  ...  \\\n",
       "0            1.0  Real Bed            flexible          True   LA  ...   \n",
       "1            1.0  Real Bed            moderate          True  NYC  ...   \n",
       "2            1.0  Real Bed            moderate          True   SF  ...   \n",
       "3            1.0  Real Bed              strict          True   LA  ...   \n",
       "4            1.0  Real Bed            flexible          True  NYC  ...   \n",
       "...          ...       ...                 ...           ...  ...  ...   \n",
       "25453        2.0  Real Bed              strict          True   SF  ...   \n",
       "25454        2.5  Real Bed            flexible         False   LA  ...   \n",
       "25455        1.0  Real Bed            moderate          True   LA  ...   \n",
       "25456        1.0  Real Bed            moderate          True  NYC  ...   \n",
       "25457        1.0  Real Bed              strict         False  NYC  ...   \n",
       "\n",
       "        latitude   longitude  \\\n",
       "0      34.028372 -118.494449   \n",
       "1      40.720380  -73.942329   \n",
       "2      37.785434 -122.470284   \n",
       "3      33.976026 -118.463471   \n",
       "4      40.735573  -74.005996   \n",
       "...          ...         ...   \n",
       "25453  37.739624 -122.414839   \n",
       "25454  34.148955 -118.368865   \n",
       "25455  34.086756 -118.353835   \n",
       "25456  40.760537  -73.989708   \n",
       "25457  40.688196  -73.945351   \n",
       "\n",
       "                                                    name       neighbourhood  \\\n",
       "0            Santa Monica Private Bedroom/Bathroom Suite        Santa Monica   \n",
       "1      Bright, charming luxury 1 BR with amazing rooftop        Williamsburg   \n",
       "2                     Private room in charming apartment   Richmond District   \n",
       "3                       Marina del Rey Beach Jr 1 Bdrm 5      Marina Del Rey   \n",
       "4                      Bright Studio Loft Prime Location        West Village   \n",
       "...                                                  ...                 ...   \n",
       "25453                 Sunny Home in Prime Bernal Heights      Bernal Heights   \n",
       "25454                       Luxury Apartment Toluca Lake         Toluca Lake   \n",
       "25455           WeHo House with Private Yard and Parking      West Hollywood   \n",
       "25456         Large 1BR in the Heart of Hell's Kitchen!!      Hell's Kitchen   \n",
       "25457                 Room w/ Private Bath in Large Apt.  Bedford-Stuyvesant   \n",
       "\n",
       "      number_of_reviews review_scores_rating  \\\n",
       "0                     6                 97.0   \n",
       "1                     2                 80.0   \n",
       "2                     2                100.0   \n",
       "3                     7                 94.0   \n",
       "4                     0                  NaN   \n",
       "...                 ...                  ...   \n",
       "25453                 8                 90.0   \n",
       "25454                 0                  NaN   \n",
       "25455                11                 95.0   \n",
       "25456                 0                  NaN   \n",
       "25457                 4                 95.0   \n",
       "\n",
       "                                           thumbnail_url zipcode  bedrooms  \\\n",
       "0      https://a0.muscache.com/im/pictures/92355eae-b...   90403       1.0   \n",
       "1      https://a0.muscache.com/im/pictures/da03e413-d...   11222       1.0   \n",
       "2      https://a0.muscache.com/im/pictures/0ba7d8aa-9...   94118       1.0   \n",
       "3      https://a0.muscache.com/im/pictures/30279741/4...   90292       0.0   \n",
       "4      https://a0.muscache.com/im/pictures/9384e262-8...   10014       1.0   \n",
       "...                                                  ...     ...       ...   \n",
       "25453  https://a0.muscache.com/im/pictures/13111646/d...   94110       3.0   \n",
       "25454                                                NaN   91602       1.0   \n",
       "25455  https://a0.muscache.com/im/pictures/12249249-f...   90046       1.0   \n",
       "25456  https://a0.muscache.com/im/pictures/f0ef6496-e...   10036       1.0   \n",
       "25457  https://a0.muscache.com/im/pictures/96846ebf-b...   11221       1.0   \n",
       "\n",
       "       beds  \n",
       "0       1.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       1.0  \n",
       "4       1.0  \n",
       "...     ...  \n",
       "25453   4.0  \n",
       "25454   1.0  \n",
       "25455   1.0  \n",
       "25456   3.0  \n",
       "25457   1.0  \n",
       "\n",
       "[25458 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open('train.csv',mode='r', encoding=\"ISO-8859-1\") as csv_file:\n",
    "    train_df = pd.read_csv(csv_file)\n",
    "\n",
    "with open('test.csv',mode='r', encoding=\"ISO-8859-1\") as csv_file:\n",
    "    test_df = pd.read_csv(csv_file)\n",
    "train_df\n",
    "test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\New folder\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m regexptokenizer(train_df)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mregexptokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m normalize_contractions(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#train_df['text'] = spell_correction(train_df['text'])\u001b[39;00m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mregexptokenizer\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregexptokenizer\u001b[39m(data):\n\u001b[1;32m----> 2\u001b[0m   data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;66;03m#remove words which are starts with @ symbols\u001b[39;00m\n\u001b[0;32m      4\u001b[0m   data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x:re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw*\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mstr\u001b[39m(x)))\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\pandas\\core\\series.py:4237\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg, na_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4163\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4235\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4239\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4240\u001b[0m     )\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\pandas\\core\\base.py:880\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# mapper is a function\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mregexptokenizer.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregexptokenizer\u001b[39m(data):\n\u001b[1;32m----> 2\u001b[0m   data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_text())\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;66;03m#remove words which are starts with @ symbols\u001b[39;00m\n\u001b[0;32m      4\u001b[0m   data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x:re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw*\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mstr\u001b[39m(x)))\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\bs4\\__init__.py:313\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    314\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    316\u001b[0m ):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "regexptokenizer(train_df)\n",
    "regexptokenizer(test_df)\n",
    "train_df['text'] = normalize_contractions(train_df['text'])\n",
    "#train_df['text'] = spell_correction(train_df['text'])\n",
    "test_df['text'] = normalize_contractions(test_df['text'])\n",
    "#test_df['text'] = spell_correction(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['Apple', 'Banana', 'Mango', 'Orange', 'Avocado']\n",
    "print([ fruit for fruit in fruits if 'n' in fruit ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
